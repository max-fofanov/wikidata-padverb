# wikidata-claims-access-python
### Способы достать информацию с wikidata

* __wikidata API__
* специализированные библиотеки
* скрэппинг __html-файла__
* __json__ dumps

#### Wikidata API
Данная схема реализована в файле [requests-to-wikidata.py](https://github.com/max-fofanov/wikidata-claims-access-python/blob/main/requests-to-wikidata.py).

1. C помощью __GET-запроса__ получаем id искомой сущности в системе __wikidata__
2. С помощью второго __GET-запроса__ получаем контент страницы в формате __json__, который затем обрабатывается средствами языка python.

#### Специализированные библиотеки

Тысячи их. Все по сути реализуют взаимодействие описанное выше, но где-то под капотом, степень настраиваемости зависит от библиотеки.

#### Скрэппинг html-файла

1. C помощью __GET-запроса__ получаем ссылку на страницу искомой сущности в системе __wikidata__.
2. Выкачиваем __html__ любым удобным способом и режем его по желанию, важно обратить внимание, что такой подход не требует конвертации из/в __wikidata id__, так как вся необходимая информация лежит в __html__ в виде текста.

#### json dumps

__Wikidata__ предоставляет возможность выкачать всё своё содержимое в виде дампа, обновляется раз в неделю. Способ получения информации не является рациональным, так как люди вообще составляют очень малую долю сущностей __wikidata__, что уж говорить о конкретных личностях, которые интересуют нас в рамках __padverb__.
